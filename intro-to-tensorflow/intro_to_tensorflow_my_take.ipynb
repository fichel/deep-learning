{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Simple NN for evaluating notMNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do this exercise we will need to:\n",
    "\n",
    "1. Prepare Data\n",
    "2. Preprocess Data\n",
    "3. Build Neural Network\n",
    "4. Train Neural Network\n",
    "5. Test Neural Network\n",
    "\n",
    "We will start by downloading our dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported\n"
     ]
    }
   ],
   "source": [
    "# standard python libraries\n",
    "import hashlib # for asserting on file checksum\n",
    "import os # for navigating through directories\n",
    "import pickle # for serializing python objects\n",
    "from urllib.request import urlretrieve # for downloading dataset\n",
    "from zipfile import ZipFile # for reading zip files\n",
    "\n",
    "# third party libraries\n",
    "import numpy as np # numerical array library\n",
    "from PIL import Image # Python Image Library, for loading images\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split # for splitting data into train/validation\n",
    "from sklearn.preprocessing import LabelBinarizer # for one-hot encoding\n",
    "from sklearn.utils import resample # for randomn sampling from dataset\n",
    "from tqdm import tqdm # progress meter library\n",
    "\n",
    "print('All modules imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "In this fase we will:\n",
    "\n",
    "1. Download the notMNIST dataset\n",
    "2. Uncompress features and labels\n",
    "3. Randomnly sample subset of 150000 images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded\n"
     ]
    }
   ],
   "source": [
    "def download(url, file):\n",
    "    # check if file was already downloaded\n",
    "    if not os.path.isfile(file):\n",
    "        # if not, then download file\n",
    "        print('Downloading {}...'.format(file))\n",
    "        urlretrieve(url, file)\n",
    "        print('Download Finished')\n",
    "\n",
    "# download the training and test dataset\n",
    "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_train.zip', 'train.zip')\n",
    "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_test.zip', 'test.zip')\n",
    "\n",
    "# check if files are corrupted\n",
    "assert hashlib.md5(open('train.zip', 'rb').read()).hexdigest() == 'c8673b3f28f489e9cdf3a3d74e2ac8fa', 'File is corrupted, download it again'\n",
    "assert hashlib.md5(open('test.zip', 'rb').read()).hexdigest() == '5d3c7e653e63471c88df796156a9dfa9', 'File is corrupted, download it again'\n",
    "\n",
    "print('All files downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210001/210001 [00:32<00:00, 6440.30files/s]\n",
      "100%|██████████| 10001/10001 [00:01<00:00, 6633.29files/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features and labels uncompressed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def uncompress_features_labels(file):\n",
    "    features = []\n",
    "    labels = []\n",
    "    with ZipFile(file) as zipfile:\n",
    "        # Progress Bar - ZipFile.namelist() returns a list of files in the zip archive\n",
    "        file_progress_bar = tqdm(zipfile.namelist(), unit='files')\n",
    "        \n",
    "        # loop through files\n",
    "        for file in file_progress_bar:\n",
    "            # check if file is not a directory\n",
    "            if not file.endswith('/'):\n",
    "                # convert features to images\n",
    "                with zipfile.open(file) as image_file:\n",
    "                    # open and load image\n",
    "                    image = Image.open(image_file)\n",
    "                    image.load()\n",
    "                    # transform image into nparray and flatten the image to a 1 dimensional array, float32\n",
    "                    feature = np.array(image, dtype=np.float32).flatten()\n",
    "                \n",
    "                # extract labels from file name\n",
    "                # file is in format train/A34.png\n",
    "                # if we split, we get ['train', 'A34.png']\n",
    "                # we want the second element ([1]) and the first character ([0])\n",
    "                label = os.path.split(file)[1][0]\n",
    "                \n",
    "                # append label and feature to array\n",
    "                features.append(feature)\n",
    "                labels.append(label)\n",
    "                \n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# get the features and labels from the zip files\n",
    "train_features, train_labels = uncompress_features_labels('train.zip')\n",
    "test_features, test_labels = uncompress_features_labels('test.zip')\n",
    "\n",
    "print('All features and labels uncompressed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomn subset sampled for training data\n"
     ]
    }
   ],
   "source": [
    "# limit the amount of training data to work with\n",
    "sample_size = 150000\n",
    "train_features, train_labels = resample(train_features, train_labels, n_samples=sample_size, replace=False, random_state=123)\n",
    "\n",
    "print('Random subset sampled for training data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Now, we will preprocess the data by doing:\n",
    "\n",
    "1. Normalize features and labels\n",
    "2. One-Hot Encode labels\n",
    "3. Randomize and split datasets for training and validation\n",
    "4. Checkpoint: Serialize all features and labels\n",
    "\n",
    "Min-Max Scaling:\n",
    "$\n",
    "X'=a+{\\frac {\\left(X-X_{\\min }\\right)\\left(b-a\\right)}{X_{\\max }-X_{\\min }}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing features normalized\n"
     ]
    }
   ],
   "source": [
    "def normalize_min_max_scaling(image_data):\n",
    "    a, b = 0.1, 0.9\n",
    "    normalized_image = a + ((image_data - 0) * (b - a))/(255 - 0)\n",
    "    return normalized_image\n",
    "\n",
    "train_features = normalize_min_max_scaling(train_features)\n",
    "test_features = normalize_min_max_scaling(test_features)\n",
    "\n",
    "print('Training and testing features normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and test labels one-hot encoded\n"
     ]
    }
   ],
   "source": [
    "# one-hot enconde labels\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_labels)\n",
    "train_labels = encoder.transform(train_labels)\n",
    "test_labels = encoder.transform(test_labels)\n",
    "\n",
    "# change label type to float32\n",
    "train_labels = train_labels.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.float32)\n",
    "\n",
    "print('Training and test labels one-hot encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features and labels randomized and split\n"
     ]
    }
   ],
   "source": [
    "# randomize and split dataset for training and validation\n",
    "train_features, valid_features, train_labels, valid_labels = train_test_split(train_features, train_labels, test_size=0.05, random_state=832289)\n",
    "\n",
    "print('Training features and labels randomized and split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to pickle file...\n",
      "Data cached in pickle file\n"
     ]
    }
   ],
   "source": [
    "# serialize and save data for access\n",
    "pickle_file = 'myModel.pickle'\n",
    "if not os.path.isfile(pickle_file):\n",
    "    print('Saving data to pickle file...')\n",
    "    try:\n",
    "        with open(pickle_file, 'wb') as pfile:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    'train_dataset': train_features,\n",
    "                    'train_labels': train_labels,\n",
    "                    'valid_dataset': valid_features,\n",
    "                    'valid_labels': valid_labels,\n",
    "                    'test_dataset': test_features,\n",
    "                    'test_labels': test_labels\n",
    "                }, pfile, pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to {}: {}'.format(pickle_file, e))\n",
    "        raise\n",
    "        \n",
    "print('Data cached in pickle file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1         0.61450982  0.38235295 ...,  0.1         0.1         0.1       ]\n",
      " [ 0.1         0.10627451  0.1        ...,  0.14078432  0.1         0.10627451]\n",
      " [ 0.1         0.1         0.1        ...,  0.1         0.1         0.1       ]\n",
      " [ 0.21607843  0.78705883  0.86862749 ...,  0.1         0.1         0.1       ]\n",
      " [ 0.1         0.1         0.1        ...,  0.1         0.1         0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "print(train_features[:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
