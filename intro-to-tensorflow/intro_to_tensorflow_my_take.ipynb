{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Simple NN for evaluating notMNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do this exercise we will need to:\n",
    "\n",
    "1. Prepare Data\n",
    "2. Preprocess Data\n",
    "3. Build Neural Network\n",
    "4. Train Neural Network\n",
    "5. Test Neural Network\n",
    "\n",
    "We will start by downloading our dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported\n"
     ]
    }
   ],
   "source": [
    "# standard python libraries\n",
    "import hashlib # for asserting on file checksum\n",
    "import os # for navigating through directories\n",
    "import pickle # for serializing python objects\n",
    "from urllib.request import urlretrieve # for downloading dataset\n",
    "from zipfile import ZipFile # for reading zip files\n",
    "\n",
    "# third party libraries\n",
    "import numpy as np # numerical array library\n",
    "from PIL import Image # Python Image Library, for loading images\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split # for splitting data into train/validation\n",
    "from sklearn.preprocessing import LabelBinarizer # for one-hot encoding\n",
    "from sklearn.utils import resample # for randomn sampling from dataset\n",
    "from tqdm import tqdm # progress meter library\n",
    "\n",
    "print('All modules imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "In this fase we will:\n",
    "\n",
    "1. Download the notMNIST dataset\n",
    "2. Uncompress features and labels\n",
    "3. Randomnly sample subset of 150000 images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded\n"
     ]
    }
   ],
   "source": [
    "def download(url, file):\n",
    "    # check if file was already downloaded\n",
    "    if not os.path.isfile(file):\n",
    "        # if not, then download file\n",
    "        print('Downloading {}...'.format(file))\n",
    "        urlretrieve(url, file)\n",
    "        print('Download Finished')\n",
    "\n",
    "# download the training and test dataset\n",
    "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_train.zip', 'train.zip')\n",
    "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_test.zip', 'test.zip')\n",
    "\n",
    "# check if files are corrupted\n",
    "assert hashlib.md5(open('train.zip', 'rb').read()).hexdigest() == 'c8673b3f28f489e9cdf3a3d74e2ac8fa', 'File is corrupted, download it again'\n",
    "assert hashlib.md5(open('test.zip', 'rb').read()).hexdigest() == '5d3c7e653e63471c88df796156a9dfa9', 'File is corrupted, download it again'\n",
    "\n",
    "print('All files downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210001/210001 [00:32<00:00, 6440.30files/s]\n",
      "100%|██████████| 10001/10001 [00:01<00:00, 6633.29files/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features and labels uncompressed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def uncompress_features_labels(file):\n",
    "    features = []\n",
    "    labels = []\n",
    "    with ZipFile(file) as zipfile:\n",
    "        # Progress Bar - ZipFile.namelist() returns a list of files in the zip archive\n",
    "        file_progress_bar = tqdm(zipfile.namelist(), unit='files')\n",
    "        \n",
    "        # loop through files\n",
    "        for file in file_progress_bar:\n",
    "            # check if file is not a directory\n",
    "            if not file.endswith('/'):\n",
    "                # convert features to images\n",
    "                with zipfile.open(file) as image_file:\n",
    "                    # open and load image\n",
    "                    image = Image.open(image_file)\n",
    "                    image.load()\n",
    "                    # transform image into nparray and flatten the image to a 1 dimensional array, float32\n",
    "                    feature = np.array(image, dtype=np.float32).flatten()\n",
    "                \n",
    "                # extract labels from file name\n",
    "                # file is in format train/A34.png\n",
    "                # if we split, we get ['train', 'A34.png']\n",
    "                # we want the second element ([1]) and the first character ([0])\n",
    "                label = os.path.split(file)[1][0]\n",
    "                \n",
    "                # append label and feature to array\n",
    "                features.append(feature)\n",
    "                labels.append(label)\n",
    "                \n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# get the features and labels from the zip files\n",
    "train_features, train_labels = uncompress_features_labels('train.zip')\n",
    "test_features, test_labels = uncompress_features_labels('test.zip')\n",
    "\n",
    "print('All features and labels uncompressed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomn subset sampled for training data\n"
     ]
    }
   ],
   "source": [
    "# limit the amount of training data to work with\n",
    "sample_size = 150000\n",
    "train_features, train_labels = resample(train_features, train_labels, n_samples=sample_size, replace=False, random_state=123)\n",
    "\n",
    "print('Random subset sampled for training data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Now, we will preprocess the data by doing:\n",
    "\n",
    "1. Normalize features\n",
    "2. One-Hot Encode labels\n",
    "3. Randomize and split datasets for training and validation\n",
    "4. Checkpoint: Serialize all features and labels\n",
    "\n",
    "Min-Max Scaling:\n",
    "$\n",
    "X'=a+{\\frac {\\left(X-X_{\\min }\\right)\\left(b-a\\right)}{X_{\\max }-X_{\\min }}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing features normalized\n"
     ]
    }
   ],
   "source": [
    "def normalize_min_max_scaling(image_data):\n",
    "    a, b = 0.1, 0.9\n",
    "    normalized_image = a + ((image_data - 0) * (b - a))/(255 - 0)\n",
    "    return normalized_image\n",
    "\n",
    "train_features = normalize_min_max_scaling(train_features)\n",
    "test_features = normalize_min_max_scaling(test_features)\n",
    "\n",
    "print('Training and testing features normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and test labels one-hot encoded\n"
     ]
    }
   ],
   "source": [
    "# one-hot enconde labels\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_labels)\n",
    "train_labels = encoder.transform(train_labels)\n",
    "test_labels = encoder.transform(test_labels)\n",
    "\n",
    "# change label type to float32\n",
    "train_labels = train_labels.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.float32)\n",
    "\n",
    "print('Training and test labels one-hot encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features and labels randomized and split\n"
     ]
    }
   ],
   "source": [
    "# randomize and split dataset for training and validation\n",
    "train_features, valid_features, train_labels, valid_labels = train_test_split(train_features, train_labels, test_size=0.05, random_state=832289)\n",
    "\n",
    "print('Training features and labels randomized and split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to pickle file...\n",
      "Data cached in pickle file\n"
     ]
    }
   ],
   "source": [
    "# serialize and save data for access\n",
    "pickle_file = 'myModel.pickle'\n",
    "if not os.path.isfile(pickle_file):\n",
    "    print('Saving data to pickle file...')\n",
    "    try:\n",
    "        with open(pickle_file, 'wb') as pfile:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    'train_dataset': train_features,\n",
    "                    'train_labels': train_labels,\n",
    "                    'valid_dataset': valid_features,\n",
    "                    'valid_labels': valid_labels,\n",
    "                    'test_dataset': test_features,\n",
    "                    'test_labels': test_labels\n",
    "                }, pfile, pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to {}: {}'.format(pickle_file, e))\n",
    "        raise\n",
    "        \n",
    "print('Data cached in pickle file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1         0.61450982  0.38235295 ...,  0.1         0.1         0.1       ]\n",
      " [ 0.1         0.10627451  0.1        ...,  0.14078432  0.1         0.10627451]\n",
      " [ 0.1         0.1         0.1        ...,  0.1         0.1         0.1       ]\n",
      " [ 0.21607843  0.78705883  0.86862749 ...,  0.1         0.1         0.1       ]\n",
      " [ 0.1         0.1         0.1        ...,  0.1         0.1         0.1       ]]\n"
     ]
    }
   ],
   "source": [
    "print(train_features[:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --> Checkpoint: Load pickle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data and modules loaded.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Load the modules\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reload the data\n",
    "pickle_file = 'myModel.pickle'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  pickle_data = pickle.load(f)\n",
    "  train_features = pickle_data['train_dataset']\n",
    "  train_labels = pickle_data['train_labels']\n",
    "  valid_features = pickle_data['valid_dataset']\n",
    "  valid_labels = pickle_data['valid_labels']\n",
    "  test_features = pickle_data['test_dataset']\n",
    "  test_labels = pickle_data['test_labels']\n",
    "  del pickle_data  # Free up memory\n",
    "\n",
    "print('Data and modules loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.2\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# initialization\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.truncated_normal([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# model\n",
    "Y = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# loss\n",
    "cross_entropy = -tf.reduce_sum(Y_ * tf.log(Y), axis=1)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# metrics\n",
    "is_correct = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Training Accuracy: 0.15397192537784576 / Training Loss: 12.262728691101074\n",
      "Validation Accuracy: 0.15479999780654907 / Validation Loss: 12.329452514648438\n",
      "\n",
      "Epoch 1\n",
      "Training Accuracy: 0.7506386041641235 / Training Loss: 1.554160475730896\n",
      "Validation Accuracy: 0.7465333342552185 / Validation Loss: 1.5511505603790283\n",
      "\n",
      "Epoch 2\n",
      "Training Accuracy: 0.7506386041641235 / Training Loss: 1.554160475730896\n",
      "Validation Accuracy: 0.7465333342552185 / Validation Loss: 1.5511505603790283\n",
      "\n",
      "Epoch 3\n",
      "Training Accuracy: 0.7506386041641235 / Training Loss: 1.554160475730896\n",
      "Validation Accuracy: 0.7465333342552185 / Validation Loss: 1.5511505603790283\n",
      "\n",
      "Epoch 4\n",
      "Training Accuracy: 0.7506386041641235 / Training Loss: 1.554160475730896\n",
      "Validation Accuracy: 0.7465333342552185 / Validation Loss: 1.5511505603790283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_next_batch(features, labels, iter_step, batch_size):\n",
    "    assert len(features) == len(labels), 'features and labels must have the same size'\n",
    "    begin = iter_step * batch_size\n",
    "    end = begin + batch_size\n",
    "    return features[begin:end], labels[begin:end]\n",
    "\n",
    "# initialize session and variables\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "train_data = {X: train_features, Y_:train_labels}\n",
    "valid_data = {X: valid_features, Y_:valid_labels}\n",
    "\n",
    "epoch_size = math.ceil(len(train_features) / batch_size)\n",
    "iterations = epoch_size * epochs\n",
    "for i in range(iterations):\n",
    "    # get next batch of train data\n",
    "    batch_X, batch_Y = get_next_batch(train_features, train_labels, i, batch_size)\n",
    "    train_batch = {X: batch_X, Y_:batch_Y}\n",
    "    \n",
    "    # run train step\n",
    "    sess.run(train_step, feed_dict=train_batch)\n",
    "    \n",
    "    # print results every epoch\n",
    "    if i % epoch_size == 0:\n",
    "        \n",
    "        # determine train accuracy and loss\n",
    "        train_acc, train_loss = sess.run([accuracy, loss], feed_dict=train_data)\n",
    "    \n",
    "        # determine validation accuracy\n",
    "        valid_acc, valid_loss = sess.run([accuracy, loss], feed_dict=valid_data)\n",
    "    \n",
    "        epoch = i / epoch_size\n",
    "        print('Epoch {}'.format(int(epoch)))\n",
    "        print('Training Accuracy: {} / Training Loss: {}'.format(train_acc, train_loss))\n",
    "        print('Validation Accuracy: {} / Validation Loss: {}\\n'.format(valid_acc, valid_loss)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8228999972343445 / Test Loss: 0.9720494151115417/n/n\n"
     ]
    }
   ],
   "source": [
    "# define test feed_dictionary\n",
    "test_data = {X: test_features, Y_:test_labels}\n",
    "\n",
    "# run nn against test data\n",
    "test_acc, test_loss = sess.run([accuracy, loss], feed_dict=test_data)    \n",
    "print('Test Accuracy: {} / Test Loss: {}/n/n'.format(test_acc, test_loss)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
